---
title: 关于Kafka Topic分区和Replication分配的策略
comments: true
cover: /gallery/defaultCover6.png
thumbnail: /gallery/defaultThumbnail6.png
tags:
  - Kafka
  - 分区
categories:
-
  - SpringCloud
  - Kafka
date: 2024-07-14 19:34:55
description:
---



# 1. Topic多分区

![ff10d773-97f3-4346-a691-230abee086f0](./2024-07-14-19-34-53/ff10d773-97f3-4346-a691-230abee086f0.png)

如图，是一个多分区Topic在Kafka集群中可能得分配情况。

P0-RL代表分区0，Leader副本。



这个Topic是3分区2副本的配置。分区尽量均匀分在不同的Broker上，分区的Follower副本尽量不和Leader在一个Broker上。



# 2. 理想的策略

假设有3个Topic在含有3个Broker的Kafka集群上。

Topic1有1个分区，2个副本。

Topic2有2个分区，2个副本。

Topic3有3个分区，2个副本。

可能如下图所示。不同颜色表示不同Topic。

![73ee8504-d50d-4b45-8fd8-2fa9714c4b6a](./2024-07-14-19-34-53/73ee8504-d50d-4b45-8fd8-2fa9714c4b6a.png)

似乎不是特别理想，我们再调整一下，如下图

![14e9e1b6-bd3a-4efa-9cf7-f0b7f669d17b](./2024-07-14-19-34-53/14e9e1b6-bd3a-4efa-9cf7-f0b7f669d17b.png)

不仅每个Broker的副本数一样了，更关键的是，并且每个Broker的主Leader副本也一样的。这样更适合负载均衡。



# 3. 实际的策略

我们使用Kafka tool，来以此创建上述3个Topic。

![d21e0312-36e4-42b9-b9e7-3a591715fe64](./2024-07-14-19-34-53/d21e0312-36e4-42b9-b9e7-3a591715fe64.png)

![59238299-c45b-4d50-87a7-046d26299fc0](./2024-07-14-19-34-53/59238299-c45b-4d50-87a7-046d26299fc0.png)

![7c1b245d-5d2e-4896-9858-bed2e9e61e35](./2024-07-14-19-34-53/7c1b245d-5d2e-4896-9858-bed2e9e61e35.png)

首先看test1

![be07d12c-5151-4daf-9e48-d091c3146f3d](./2024-07-14-19-34-53/be07d12c-5151-4daf-9e48-d091c3146f3d.png)

![defe1328-65bb-489f-a29c-c191226f7dca](./2024-07-14-19-34-53/defe1328-65bb-489f-a29c-c191226f7dca.png)

然后看test2

![715536d6-6ff2-4733-90cf-118c6d9d14d1](./2024-07-14-19-34-53/715536d6-6ff2-4733-90cf-118c6d9d14d1.png)

![f2c91208-0827-44eb-8d4c-889eeda5fde4](./2024-07-14-19-34-53/f2c91208-0827-44eb-8d4c-889eeda5fde4.png)

![1827b793-8c1c-486c-bda6-3bcaca5a4cee](./2024-07-14-19-34-53/1827b793-8c1c-486c-bda6-3bcaca5a4cee.png)

![c66c779a-037a-48f6-8394-6df9a6c00412](./2024-07-14-19-34-53/c66c779a-037a-48f6-8394-6df9a6c00412.png)

然后是test3

![3338a49e-0c29-4e12-bbbb-447e10d2eaba](./2024-07-14-19-34-53/3338a49e-0c29-4e12-bbbb-447e10d2eaba.png)

![8b88dfd5-77d2-41e6-90f3-718c08c3530d](./2024-07-14-19-34-53/8b88dfd5-77d2-41e6-90f3-718c08c3530d.png)

![6fdba615-70bb-4b62-9b52-2ea55f9d8704](./2024-07-14-19-34-53/6fdba615-70bb-4b62-9b52-2ea55f9d8704.png)

![8f9305c8-d285-41ce-a844-e035b7826c00](./2024-07-14-19-34-53/8f9305c8-d285-41ce-a844-e035b7826c00.png)

![af242098-99f5-49dd-934e-fef4a9abcbf1](./2024-07-14-19-34-53/af242098-99f5-49dd-934e-fef4a9abcbf1.png)

![0e153c2d-cb07-456c-8d31-272f526f9664](./2024-07-14-19-34-53/0e153c2d-cb07-456c-8d31-272f526f9664.png)

按照上面的信息，画出来的分配结果如下图

![c5c1c3dd-c9ef-4700-a569-e9cb499d2711](./2024-07-14-19-34-53/c5c1c3dd-c9ef-4700-a569-e9cb499d2711.png)

似乎并不和我们想的一样。



查看源码，`Breadcrumbskafka/server-common/src/main/java/org/apache/kafka/admin/AdminUtils.java`中一段代码

```java
private static Map<Integer, List<Integer>> assignReplicasToBrokersRackUnaware(int nPartitions,
                                                                                  int replicationFactor,
                                                                                  List<Integer> brokerList,
                                                                                  int fixedStartIndex,
                                                                                  int startPartitionId) {
        Map<Integer, List<Integer>> ret = new HashMap<>();
        int startIndex = fixedStartIndex >= 0 ? fixedStartIndex : RAND.nextInt(brokerList.size());
        int currentPartitionId = Math.max(0, startPartitionId);
        int nextReplicaShift = fixedStartIndex >= 0 ? fixedStartIndex : RAND.nextInt(brokerList.size());
        for (int i = 0; i < nPartitions; i++) {
            if (currentPartitionId > 0 && (currentPartitionId % brokerList.size() == 0))
                nextReplicaShift += 1;
            int firstReplicaIndex = (currentPartitionId + startIndex) % brokerList.size();
            List<Integer> replicaBuffer = new ArrayList<>();
            replicaBuffer.add(brokerList.get(firstReplicaIndex));
            for (int j = 0; j < replicationFactor - 1; j++)
                replicaBuffer.add(brokerList.get(replicaIndex(firstReplicaIndex, nextReplicaShift, j, brokerList.size())));
            ret.put(currentPartitionId, replicaBuffer);
            currentPartitionId += 1;
        }
        return ret;
    }
```

例子（来自尚硅谷）

![cd879db0-0daa-49f5-ae86-89d2f1472637](./2024-07-14-19-34-53/cd879db0-0daa-49f5-ae86-89d2f1472637.png)

![6e020963-c2b1-4d7f-b618-53dc2d923356](./2024-07-14-19-34-53/6e020963-c2b1-4d7f-b618-53dc2d923356.png)

![f2a7ee0a-29b8-4fdc-97f0-d3bb097943a3](./2024-07-14-19-34-53/f2a7ee0a-29b8-4fdc-97f0-d3bb097943a3.png)



# 4. 如何自定义策略

```java
public class AdminTopicTest {
    public static void main(String[] args) {
        //定义kafka集群配置
        Map<String, Object> config = new HashMap<>();
        config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:19092");

        //创建Admin管理员对象
        Admin admin = Admin.create(config);

        //定义Topic属性
        HashMap<Integer, List<Integer>> map = new HashMap<>();
        // 分区0，Leader副本在3上，第二个副本在1上。
        map.put(0, Arrays.asList(3, 1));
        map.put(1, Arrays.asList(2, 3));
        map.put(2, Arrays.asList(1, 2));
        NewTopic test4 = new NewTopic("test2", map);


        //创建Topic
        CreateTopicsResult result = admin.createTopics(
                Arrays.asList(
                        test4
                )
        );

        admin.close();
    }
}
```

不过在手动分配时，确实需要了解每个broker的负载情况，以便做出更优的分配策略。你可以使用Kafka的`AdminClient`类来获取集群的状态信息


