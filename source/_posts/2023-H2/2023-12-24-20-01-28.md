---
layout: post
title: Docker单点部署[8.11.3] Elasticsearch + Kibana + ik分词器 + pinyin分词器
date: 2023-12-24 20:01:28
author: 'Xiamu'
cover: /2023/12/24/2023-H2/2023-12-24-20-01-28/1718889656745.png
thumbnail: /2023/12/24/2023-H2/2023-12-24-20-01-28/1718889656745.png
tags:
- docker
- elasticsearch
- jenkins
categories:
- 
  - SpringCloud
  - Elasticsearch

---


这里记录一次成功简单登陆`Kibana`的实际经验。

# 一、Elasticsearch

运行`Elasticsearch`容器

```prism language-shell
docker run -d \
	--name es \
	-e "ES_JAVA_OPTS=-Xms512m -Xmx512m" \
	-e "discovery.type=single-node" \
	-e "xpack.security.enabled=true" \
	-e "xpack.security.enrollment.enabled=true" \
	-v your_host_es_data_path:/usr/share/elasticsearch/data \ #宿主机绝对路径挂载
	-v your_host_es_plugins_path:/usr/share/elasticsearch/plugins \ #宿主机绝对路径挂载
	--privileged \
	--network es-net \
	-p 9200:9200 \
	-p 9300:9300 \
elasticsearch:8.11.3
```

重置`elastic`密码，记住这段密码

```prism language-shell
docker exec -it es /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic
```

重置`kibana_system` 密码，记住这段密码

```prism language-shell
docker exec -it es /usr/share/elasticsearch/bin/elasticsearch-reset-password -u kibana_system 
```

# 二、Kibana

运行`Kibana`容器，账户密码填`kibana_system` 的

```prism language-shell
docker run -d \
	--name kibana \
	-e ELASTICSEARCH_HOSTS=http://es:9200 \
	-e ELASTICSEARCH_USERNAME=kibana_system \
	-e ELASTICSEARCH_PASSWORD=kibana_system_passwrod \ #刚才获得的kibana_system 密码
	--network=es-net \
	-p 5601:5601  \
kibana:8.11.3
```

# 三、访问

访问`http://localhost:5601`  
用`elastic`的账号密码登录。  
{% asset_img 1718889655466.png This is an example image %}

# 四、其他

关于一些报错

1. `kibana`容器创建时不允许用`elastic`用户连接`elasticsearch`
2. 运行`docker exec -it es01 /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana`报错`SSL`错误
3. 等等各种因为使用了不是`8.11.3`版本的安全验证方法遇到的错误

这里是官方的`install with docker`教程，也是一坨shit。  
<https://www.elastic.co/guide/en/kibana/current/docker.html>

这里是官方关于安全配置的docs，遇到什么问题就多翻翻。  
<https://www.elastic.co/guide/en/elasticsearch/reference/master/manually-configure-security.html>

或者来社区多讨论讨论。  
<https://discuss.elastic.co/latest>

# 五、ik分词器

这里是官方仓库  
<https://github.com/medcl/elasticsearch-analysis-ik>  
推荐有两种安装方式

## 第一种：在线安装

```prism language-bash
# 进入容器内部
docker exec -it es /bin/bash

# 在线下载并安装

#退出
exit
#重启容器
docker restart es
docker restart kibana
```

> 如果遇到ik版本和es版本不匹配问题请看下面

## 第二种：离线安装

1. 在发行版下载页面，找到和`es`版本最接近的`ik`版本(博主这里是ik8.11.1 + es8.11.3)  
   <https://github.com/medcl/elasticsearch-analysis-ik/releases>

2. 在您的 `your_host_es_plugins_path` 目录下，创建一个名为 `ik` 的新文件夹。

3. 将下载的 `elasticsearch-analysis-ik-8.11.1.zip` 文件解压到刚刚创建的 `ik` 文件夹中  
   {% asset_img 1718889655772.png This is an example image %}

4. 修改`plugin-descriptor.properties`文件

> ***!如无需要请跳过，可能造成无法预估的bug***

```prism language-bash
# 'version': plugin's version
version=8.11.3
# 'elasticsearch.version' version of elasticsearch compiled against
# You will have to release a new version of the plugin for each new
# elasticsearch release. This version is checked when the plugin
# is loaded so Elasticsearch will refuse to start in the presence of
# plugins with the incorrect elasticsearch.version.
elasticsearch.version=8.11.3
```

5. 重启容器

```prism language-bash
docker restart es
docker restart kibana
```

安装好了之后，登录`kinaba`，找到`Dev Tools - Console`

```prism language-bash
#测试分词器
GET /_analyze
{
   
  "text":"我爱吃冰淇淋，也喜欢小淇，i want to eat her",
  "analyzer":"ik_smart"
}

#测试分词器
GET /_analyze
{
   
  "text":"我爱吃冰淇淋，也喜欢小淇，i want to eat her",
  "analyzer":"ik_max_word"
}
```

{% asset_img 1718889655958.png This is an example image %}
> 这里的句子分词ik_smart和ik_max_word区别不明显，可以换用"程序员"试试。

# 六、ik分词器的扩展和停用

## 1. 配置

ik分词器并不能准确识别最新的网络流行词，以及禁用敏感词。  
我们可以手动配置来实现这两点。  
{% asset_img 1718889656111.png This is an example image %}  
修改`IKAnalyzer.cfg.xml`文件如下

```prism language-xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 -->
	<entry key="ext_dict">ext.dic</entry>
	 <!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords">stopword.dic</entry>
	<!--用户可以在这里配置远程扩展字典 -->
	<!-- <entry key="remote_ext_dict">words_location</entry> -->
	<!--用户可以在这里配置远程扩展停止词字典-->
	<!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>

```

这里的意思是，使用同目录下的`ext.dic`作为扩展词汇；使用同目录下的`stopword.dic`作为禁用词汇。这两个文件有就用，没有就新建。  
{% asset_img 1718889656237.png This is an example image %}  
{% asset_img 1718889656317.png This is an example image %}  
最后记得重启`es`容器

## 2. 测试

```prism language-bash
#测试分词器
GET /_analyze
{
   
  "text":"程序员墨扛教育的课程可以白嫖啊，而且就业率高达95%哦，奥利给！嘤",
  "analyzer":"ik_smart"
}
```

```prism language-bash
{
   
  "tokens": [
    {
   
      "token": "程序员",
      "start_offset": 0,
      "end_offset": 3,
      "type": "CN_WORD",
      "position": 0
    },
    {
   
      "token": "墨扛教育",
      "start_offset": 3,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 1
    },
    {
   
      "token": "课程",
      "start_offset": 8,
      "end_offset": 10,
      "type": "CN_WORD",
      "position": 2
    },
    {
   
      "token": "可以",
      "start_offset": 10,
      "end_offset": 12,
      "type": "CN_WORD",
      "position": 3
    },
    {
   
      "token": "白嫖",
      "start_offset": 12,
      "end_offset": 14,
      "type": "CN_WORD",
      "position": 4
    },
    {
   
      "token": "而且",
      "start_offset": 16,
      "end_offset": 18,
      "type": "CN_WORD",
      "position": 5
    },
    {
   
      "token": "就业率",
      "start_offset": 18,
      "end_offset": 21,
      "type": "CN_WORD",
      "position": 6
    },
    {
   
      "token": "高达",
      "start_offset": 21,
      "end_offset": 23,
      "type": "CN_WORD",
      "position": 7
    },
    {
   
      "token": "95",
      "start_offset": 23,
      "end_offset": 25,
      "type": "ARABIC",
      "position": 8
    },
    {
   
      "token": "奥利给",
      "start_offset": 28,
      "end_offset": 31,
      "type": "CN_WORD",
      "position": 9
    }
  ]
}
```

# 七、pinyin分词器

## 离线安装

1. 在发行版下载页面，找到和`es`版本最接近的版本(博主这里是pinyin8.11.1 + es8.11.3)  
   <https://github.com/medcl/elasticsearch-analysis-pinyin>

2. 在您的 `your_host_es_plugins_path` 目录下，创建一个名为 `py` 的新文件夹。

3. 将下载的 `elasticsearch-analysis-pinyin-8.11.1.zip` 文件解压到刚刚创建的 `py` 文件夹中  
   {% asset_img 1718889656396.png This is an example image %}

4. 修改`plugin-descriptor.properties`文件

> ***!如无需要请跳过，可能造成无法预估的bug***

```prism language-bash
# 'version': plugin's version
version=8.11.3
# 'elasticsearch.version' version of elasticsearch compiled against
# You will have to release a new version of the plugin for each new
# elasticsearch release. This version is checked when the plugin
# is loaded so Elasticsearch will refuse to start in the presence of
# plugins with the incorrect elasticsearch.version.
elasticsearch.version=8.11.3
```

5. 重启容器

```prism language-bash
docker restart es
docker restart kibana
```

安装好了之后，登录`kinaba`，找到`Dev Tools - Console`

```prism language-bash
#测试分词器
POST /_analyze
{
   
  "text":"如家酒店还不错",
  "analyzer":"pinyin"
}
```

{% asset_img 1718889656477.png This is an example image %}

## 注意事项

`pinyin`分词器默认时有很多缺点，比如每个字都拆分变成拼音，不符合一般需求，并且如果使用`pinyin`分词器，默认的中文索引就没了，只剩下`pinyin`索引了。所以，需要完善以下几点：

1. 分词时不仅包含汉字，还需包含拼音
2. 分词时按词分，不是字
3. 使用汉字查询时，不会查询到同音词条目docs

为了做到这几点，需要在创建索引库时构建一个自定义分词器，如下

```prism language-json
PUT /test
{
   
  "settings": {
   
    "analysis": {
   
      "analyzer": {
   
        "my_analyzer":{
   
          "tokenizer":"ik_max_word",
          "filter":"py"
        }
      },
      "filter": {
   
        "py":{
   
          "type":"pinyin",
          "keep_full_pinyin":false,
          "keep_joined_full_pinyin":true,
          "keep_original":true,
          "limit_first_letter_length":16,
          "remove_duplicated_term":true,
          "none_chinese_pinyin_tokenize":false
        }
      }
    }
  },
  "mappings": {
   
    "properties": {
   
      "name":{
   
        "type": "text",
        "analyzer": "my_analyzer",
        "search_analyzer": "ik_smart"
      }
    }
  }
}
```

{% asset_img 1718889656581.png This is an example image %}  
我们自定义了三步之中的`tokenizer`和`filter`，前者用`ik`分词，后者用`pinyin`分词，同时自定义了`pinyin`分词器的一些设置，分词时同时保留汉字和拼音，具体设置看`pinyin`分词器的`github`官网。同时设定了存入数据时使用分词器`my_analyzer`，搜索时，使用分词器`ik_smart`。

存入两个数据，如下

```prism language-json
POST /test/_doc/1
{
   
  "id":1,
  "name":"狮子"
}

POST /test/_doc/2
{
   
  "id":2,
  "name":"虱子"
}
```

那么现在，索引库的具体内容如下所示{% asset_img 1718889656745.png This is an example image %}  
因为搜索时使用的是`ik_smart`分词器，不是自定义分词器，所以这里已经解决了同音词的问题。

